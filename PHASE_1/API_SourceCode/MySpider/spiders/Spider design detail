Spider design
    Implementation method:
        We used Python scrappy library to develop our scraper. The scraper consist of two spiders which are url spider
        and content spider. The url spider are used to access and cache headline with urls of news from our data source
        while the content spider are used to access content of these urls and extract report data from the main text. The
        structure of our scraper are:
            Items: Defined the object field we need to crawl from data source
            Spider: Access and extract data
            Pipelines: Formalizing the item and storage
        There are two main reasons we decided to use this structure. Firstly, we can segment the date access and storage
        processes so that we formalize our report structure easier on the pipeline. Moreover, we can store our data
        individually which avoid usage of long list
    Challenge:
        The key challenge of our scraper is extract required data from the main text. Located the data basing on appearance
        of some special word. For example, it is likely to get the affected number on the sentence with verb 'reported',
        'confirmed'. For those unsolved cases we manually extract on the pipelines process
